<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Super Resolution</title>
      <link href="/2020/02/04/super-resolution/"/>
      <url>/2020/02/04/super-resolution/</url>
      
        <content type="html"><![CDATA[<h1 id="SISR"><a href="#SISR" class="headerlink" title="SISR"></a>SISR</h1><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><h3 id="SRGAN-CVPR2017"><a href="#SRGAN-CVPR2017" class="headerlink" title="SRGAN (CVPR2017)"></a>SRGAN (CVPR2017)</h3><ul><li><p>训练网络时用均方差作为损失函数，虽然能够获得很高的峰值信噪比，但是恢复出来的图像通常会丢失高频细节，使人不能有好的视觉感受。</p></li><li><p>MSE代价函数使重建结果有较高的信噪比，但是缺少了高频信息，出现过度平滑的纹理。</p></li><li><p>损失函数</p><p><img src="image-20200227231314003.png" alt="image-20200227231314003"></p></li></ul><h3 id="ESRGAN-ECCV2018"><a href="#ESRGAN-ECCV2018" class="headerlink" title="ESRGAN (ECCV2018)"></a>ESRGAN (ECCV2018)</h3><ul><li><p>改进感知域损失函数，使用<strong>激活前</strong>的VGG特征，这个改进会提供更尖锐的边缘和更符合视觉的结果。</p></li><li><p>GAN网络改进为Relativistic average GAN (RaGAN)</p></li><li><p>网络的基本单元从基本的残差单元变为Residual-in-Residual Dense Block (RRDB)</p></li><li><p>当训练集和测试集的统计量有很大不同的时候，BN层就会倾向于生成不好的伪影，并且限制模型的泛化能力。作者发现，BN层在网络比较深，而且在GAN框架下进行训练的时候，更会产生伪影。这些伪影偶尔出现在迭代和不同的设置中，违反了对训练稳定性能的需求。所以为了稳定的训练和一致的性能，作者去掉了BN层。</p></li><li><p>对残差信息进行scaling，即将残差信息乘以一个0到1之间的数，用于防止不稳定</p></li><li><p>损失函数</p><p><img src="image-20200227231642371.png" alt="image-20200227231642371"></p></li></ul><h3 id="SROBB-ICCV-2019"><a href="#SROBB-ICCV-2019" class="headerlink" title="SROBB (ICCV 2019)"></a>SROBB (ICCV 2019)</h3><ul><li><p>mid-level信息：纹理信息</p><p>low-level信息：边缘信息</p></li><li><p>浅层网络提取感知信息：适用边缘，纹理处模糊</p><p>深层网络提取感知信息：适用纹理，图片引入噪声</p><p><strong>低层的特征对于重建边来说是更有效的，中间层的特征对于重建纹理来说是更有效的</strong></p><p><img src="image-20200228141728188.png" alt="image-20200228141728188"></p></li><li><p>分区域：<br><strong>背景：</strong>天空、植物、地面和水，这些区域主要包含的是<strong>纹理</strong>信息，用符号background表示，其对应的是<strong>mid-level</strong>特征。</p><p><strong>边界</strong>：区分物体和背景的边，这些区域主要包含的是<strong>边和块</strong>信息，作者通过处理拓宽了这些区域，用符号boundary表示，其对应的是<strong>low-level</strong>特征。</p><p><strong>物体</strong>：除了上面所说的背景所包含的4类，用符号object表示，其对应的是<strong>high-level</strong>特征。</p></li><li><p>区域区分</p><p><img src="image-20200228162410278.png" alt="image-20200228162410278"></p></li><li><p>感知损失函数</p><p><img src="image-20200228175935771.png" alt="image-20200228175935771"></p></li><li><p>网络结构（延用SRGAN）</p><p><img src="image-20200228162440113.png" alt="image-20200228162440113"></p></li></ul><h3 id="RankSRGAN-ICCV-2019"><a href="#RankSRGAN-ICCV-2019" class="headerlink" title="RankSRGAN (ICCV 2019)"></a>RankSRGAN (ICCV 2019)</h3><ul><li><p>创新点：将不可微的图像感知质量指标通过Ranker模块转化为可微可优化的值</p></li><li><p>网络架构</p><p><img src="image-20200228175302369.png" alt="image-20200228175302369"></p></li><li><p><strong>Stage1：</strong>生成成对的带有排序标签的图像（rank images）。通过不同的 SR 方法对公开的SR 数据集进行处理，生成超分辨率图像，然后对这些图像用所中的感知指标（如 NIQE）进行评价得到质量评价得分，接着根据分数对成对图像进行排序。按照得分高低，图像的排序标签分别为1~n（1表示最好，n则为最差）。这里选择的SR 方法为 ESRGAN 和 SRGAN，数据集则是 DIV2K 和 Flicker2K。</p></li><li><p><strong>Stage2：</strong>训练 Ranker，<strong>也是论文核心的创新点</strong>。论文采用了孪生神经网络结构，网络分支共享权值。同时提出了 margin-ranking loss，对 Ranker 进行优化。训练后的Ranker具有根据图像的感知得分对图像进行排名的能力。</p><p><img src="image-20200228175441824.png" alt="image-20200228175441824"></p></li><li><p><strong>Stage 3:</strong> 引入 rank-content loss。当Ranker训练完成之后，Ranker 作为一个评价网络生成 rank-content loss。生成的图像输入Ranker中以预测排名分数。rank-content loss 定义如下：</p><p><img src="image-20200228175500131.png" alt="image-20200228175500131"></p><p>最终的网络结构中，网络结构与 SRGAN 相同，不过引入了额外的 Ranker-content loss。</p></li></ul><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><h3 id="SAN-（CVPR-2019）"><a href="#SAN-（CVPR-2019）" class="headerlink" title="SAN （CVPR 2019）"></a>SAN （CVPR 2019）</h3><ul><li><p>CNN模型限制：</p><p>大多数基于CNN的SR方法没有充分利用原始LR图像的信息，导致相当低的性能</p><p>大多数CNN-based models主要专注于设计更深或是更宽的网络，以学习更有判别力的高层特征，却很少发掘层间特征的内在相关性，从而妨碍了CNN的表达能力</p></li><li><p>模型结构：</p><p><img src="image-20200302163238530.png" alt="image-20200302163238530"></p></li><li><p>NLGR: 区域级非局部模块RL-NL + 一个同源残差组结构SSRG</p><p>同源残差连接：把LR的特征加到每个group的输入x中，这种连接不仅可以帮助深度CNN的训练，同时还可以传递LR图像中丰富的低频信息给high-level的层</p></li><li><p>RL-NL模块</p><p>将图像划分为<em>k</em>x<em>k</em>大小，在每个region中进行non-local操作</p><p>local：针对感受野</p><p>conv、pooling：local</p><p>FC：non-local，global</p><p>Non-local block：<img src="image-20200302171603723.png" alt="image-20200302171603723"></p><p>mask由相似性给出</p><p>g是一个映射函数，将一个点映射成一个向量，可以看成是计算一个点的特征</p><p><img src="image-20200302171616023.png" alt="image-20200302171616023"></p></li><li><p>SSRG: G个局部模块LSRAG + 同源残差连接结构SSC</p><p>所谓同源残差连接，就是把LR的特征加到每个group的输入x中，这种连接不仅可以帮助深度CNN的训练，同时还可以传递LR图像中丰富的低频信息给high-level的层</p></li><li><p>LSRAG：</p><p>与NLGR不同，采用local的残差连接，非同源残差连接</p></li><li><p>SOCA：二阶通道注意力机制，得到通道注意力</p></li></ul><h3 id="SRFBN-（CVPR-2019）"><a href="#SRFBN-（CVPR-2019）" class="headerlink" title="SRFBN （CVPR 2019）"></a>SRFBN （CVPR 2019）</h3><ul><li><p>回传（RNN，常见的直接前向传递模块的弊端是前面的层无法直接利用后面层的信息）</p><p>循环迭代T次，每次迭代都计算loss值，T次迭代后平均loss更新权重</p><p><img src="https://i.loli.net/2019/05/07/5cd129e15e956.png" alt="img"></p></li><li><p>反馈模块（综合HR与LR信息）</p><p><img src="https://i.loli.net/2019/05/07/5cd13184b60a3.png" alt="img"></p></li><li><p>缺点：计算量过大，demo阶段仍需要多次循环迭代</p></li></ul><h3 id="Meta-SR-CVPR-2019"><a href="#Meta-SR-CVPR-2019" class="headerlink" title="Meta-SR (CVPR 2019)"></a>Meta-SR (CVPR 2019)</h3><ul><li><p>提出一个动态预测每一缩放因子的滤波器权重的新网络，从而无需为每一缩放因子存储权重，取而代之，存储小的权重预测网络更为方便</p></li><li><p>包含两个模块：特征学习模块和 Meta-Upscale 模块，后者的提出用于替代传统的放大模块</p></li><li><p>Meta-Upscale</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39xeqWvibvIAH245PzV76FTVWnIBLEjXIHibgWONwHdr3K4Aib3tiaDRNO3wkFBcfWxLQKckIlcGcA0jhQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39xeqWvibvIAH245PzV76FTVW4lNdBS0TOtvSu7JkxB7gmVib7Jg57Hz67rAKrdNTmGFSDphwCPicibXfQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>1) Location Projection： 把像素投射到 LR 图像上，即找到与像素（i, j）对应的像素（i′, j′）</p><p>​    向下取整</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39xeqWvibvIAH245PzV76FTVWvYpaWo2XoqwSHNR8xBXCQDEt7SYHdKKkia3lQIINBlbllVdTDeQceHw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>2) Weight Prediction： 为 SR 图像上每个像素预测对应滤波器的权重</p><p>​    应用meta-learning （<strong>为何 Vij 使用偏差表示？</strong>）</p><p>​                                <img src="https://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39xeqWvibvIAH245PzV76FTVWY6iawLkFwKxrL07rOUWW9Tpeujzh3zfSeY195fU6IiaInH8VQiab4iaPeQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/MR4y3Zyg39xeqWvibvIAH245PzV76FTVWu7MS06kuNRWwqAsLYHiavPPFAISSsyI8IW4qaF82UxqrNUm2yMQYdmw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>3) Feature Mapping：利用预测得到的权重将 LR 图像的特征映射回 SR 图像空间以计算其像素值</p></li></ul><h1 id="RefSR"><a href="#RefSR" class="headerlink" title="RefSR"></a>RefSR</h1><h2 id="CrossNet-ECCV-2018"><a href="#CrossNet-ECCV-2018" class="headerlink" title="CrossNet (ECCV 2018)"></a>CrossNet (ECCV 2018)</h2><ul><li><p>将参考HR图像的高频细节迁移到LR图像</p></li><li><p>本文提出一种端到端的全卷积神经网络，使用了跨尺度扭曲，包含图像编码器，跨尺度扭曲层和融合解码器：<strong>编码器</strong>主要用于提取参考图片和LR图片的多尺度特征，</p><p><strong>跨尺度变形层</strong>用于将参考的特征图和LR特征图进行空间对齐，</p><p><strong>解码器</strong>将这些特征图进行合并从而生成HR图片。</p></li><li><p>网络结构</p><p><img src="image-20200228202251954.png" alt="image-20200228202251954"></p></li><li><p>损失函数</p><p><img src="image-20200228202416602.png" alt="image-20200228202416602"></p></li></ul><h1 id="综合"><a href="#综合" class="headerlink" title="综合"></a>综合</h1><h2 id="SRNTT-CVPR-2019"><a href="#SRNTT-CVPR-2019" class="headerlink" title="SRNTT (CVPR 2019)"></a>SRNTT (CVPR 2019)</h2><ul><li><p>SISR：</p><p>纹理不够清晰</p><p>加上感知损失，纹理清晰了但是大多是捏造的，不符合真实情况</p></li><li><p>RefSR：</p><p>Ref需要跟LR足够相似</p><p>只学习了Ref的像素级特征，或者一些浅层的特征</p><p>Ref需要与LR对齐的</p></li><li><p>通过在特征空间上匹配的方法，将语义相关的特征进行迁移。</p></li><li><p>框架：包含局部纹理特征匹配（交换）和纹理迁移两部分</p><p>论文将输入图片与参考图片先转化为同一分辨率，随后将图像<strong>分块计算</strong>。</p><p><strong>swap</strong>：找到参考图片中与输入图片相似度（内积）最大的块，并替换输入图片中对应块，输入纹理转换</p><p><img src="image-20200229003557885.png" alt="image-20200229003557885"></p></li><li><p>纹理迁移</p><p><img src="image-20200229005424330.png" alt="image-20200229005424330"></p></li><li><p>损失函数</p><p>重构损失：</p><p><img src="image-20200229010026666.png" alt="image-20200229010026666"></p><p>感知损失：</p><p><img src="image-20200229010034890.png" alt="image-20200229010034890"></p><p>对抗损失：</p><p><img src="image-20200229010041668.png" alt="image-20200229010041668"></p><p>纹理损失：</p><p><img src="image-20200229010144944.png" alt="image-20200229010144944"></p></li><li><p>数据集 成对数据集（需要ref）</p><p><img src="image-20200229010259679.png" alt="image-20200229010259679"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Meta Learning （Hongyi Li）</title>
      <link href="/2020/02/04/meta-learning/"/>
      <url>/2020/02/04/meta-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Meta-Learning-Learn-2-Learn"><a href="#Meta-Learning-Learn-2-Learn" class="headerlink" title="Meta Learning = Learn 2 Learn"></a>Meta Learning = Learn 2 Learn</h3><h3 id="Life-Long-Learning-amp-Meta-Learning"><a href="#Life-Long-Learning-amp-Meta-Learning" class="headerlink" title="Life-Long Learning &amp; Meta Learning"></a>Life-Long Learning &amp; Meta Learning</h3><p>Life-Long Learning: 不断用同一个模型进行学习</p><p>Meta Learning: 不同任务拥有不同的模型</p><h3 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h3><p><img src="image-20200204142328027.png" alt="image-20200204142328027"></p><h3 id="Meta-Learning-1"><a href="#Meta-Learning-1" class="headerlink" title="Meta Learning"></a>Meta Learning</h3><p><img src="image-20200204142350120.png" alt="image-20200204142350120"></p><h3 id="Machine-Learning-amp-Meta-Learning"><a href="#Machine-Learning-amp-Meta-Learning" class="headerlink" title="Machine Learning &amp; Meta Learning"></a>Machine Learning &amp; Meta Learning</h3><p><img src="image-20200204142525193.png" alt="image-20200204142525193"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Loss函数，多任务loss值之和"><a href="#Loss函数，多任务loss值之和" class="headerlink" title="Loss函数，多任务loss值之和"></a>Loss函数，多任务loss值之和</h3><p><img src="image-20200204143128683.png" alt="image-20200204143128683"></p><h3 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h3><p>ML为训练集&amp;测试集，Meta Learning为训练任务&amp;测试任务</p><p><img src="image-20200204143322759.png" alt="image-20200204143322759"></p><h3 id="Meta-Learning常与few-shot-learning联系，仅用少量数据集进行训练"><a href="#Meta-Learning常与few-shot-learning联系，仅用少量数据集进行训练" class="headerlink" title="Meta Learning常与few-shot learning联系，仅用少量数据集进行训练"></a>Meta Learning常与few-shot learning联系，仅用少量数据集进行训练</h3><h3 id="Support-amp-Query-set"><a href="#Support-amp-Query-set" class="headerlink" title="Support &amp; Query set"></a>Support &amp; Query set</h3><p><img src="image-20200204144029253.png" alt="image-20200204144029253"></p><h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p><img src="image-20200204144126330.png" alt="image-20200204144126330"></p><h2 id="Omniglot"><a href="#Omniglot" class="headerlink" title="Omniglot"></a>Omniglot</h2><h3 id="Few-Shot-Classification"><a href="#Few-Shot-Classification" class="headerlink" title="Few-Shot Classification"></a>Few-Shot Classification</h3><p><img src="image-20200204144643335.png" alt="image-20200204144643335"></p><h2 id="常见方法"><a href="#常见方法" class="headerlink" title="常见方法"></a>常见方法</h2><h3 id="MAML"><a href="#MAML" class="headerlink" title="MAML"></a>MAML</h3><h4 id="固定模型架构，仅针对初始化参数"><a href="#固定模型架构，仅针对初始化参数" class="headerlink" title="固定模型架构，仅针对初始化参数"></a>固定模型架构，仅针对初始化参数</h4><p><img src="image-20200204145154001.png" alt="image-20200204145154001"></p><h4 id="MAML-amp-Model-Pre-Training"><a href="#MAML-amp-Model-Pre-Training" class="headerlink" title="MAML &amp; Model Pre-Training"></a>MAML &amp; Model Pre-Training</h4><p><img src="image-20200204145458988.png" alt="image-20200204145458988"><br><img src="image-20200204145530859.png" alt="image-20200204145530859"><br><img src="image-20200204145558559.png" alt="image-20200204145558559"></p><h4 id="MAML训练仅update一次，测试中为取得更好效果可update多次"><a href="#MAML训练仅update一次，测试中为取得更好效果可update多次" class="headerlink" title="MAML训练仅update一次，测试中为取得更好效果可update多次"></a>MAML训练仅update一次，测试中为取得更好效果可update多次</h4><p><img src="image-20200204145858562.png" alt="image-20200204145858562"></p><h4 id="Example：拟合三角函数"><a href="#Example：拟合三角函数" class="headerlink" title="Example：拟合三角函数"></a>Example：拟合三角函数</h4><p><img src="image-20200204150034660.png" alt="image-20200204150034660"><br><img src="image-20200204150319806.png" alt="image-20200204150319806"> </p><h4 id="数学推理"><a href="#数学推理" class="headerlink" title="数学推理"></a>数学推理</h4><p><img src="image-20200204151409192.png" alt="image-20200204151409192"></p><h4 id="first-order-approximation-近似简化计算"><a href="#first-order-approximation-近似简化计算" class="headerlink" title="first-order approximation 近似简化计算"></a>first-order approximation 近似简化计算</h4><p><img src="image-20200204151527685.png" alt="image-20200204151527685"><br><img src="image-20200204151613428.png" alt="image-20200204151613428"></p><h4 id="实际应用："><a href="#实际应用：" class="headerlink" title="实际应用："></a>实际应用：</h4><p>MAML参数进行两次update：</p><p> （1）得到训练后参数theta</p><p>  （2）得到theta对loss的偏微分，用于更新初始参数</p><p><img src="image-20200204152145493.png" alt="image-20200204152145493"></p><h3 id="Reptile"><a href="#Reptile" class="headerlink" title="Reptile"></a>Reptile</h3><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p><img src="image-20200204162459449.png" alt="image-20200204162459449"></p><h4 id="区别：Pre-train-amp-MAML-amp-Reptile"><a href="#区别：Pre-train-amp-MAML-amp-Reptile" class="headerlink" title="区别：Pre-train &amp; MAML &amp; Reptile"></a>区别：Pre-train &amp; MAML &amp; Reptile</h4><p><img src="image-20200204162516883.png" alt="image-20200204162516883"></p><h2 id="Gradient-Descent-as-LSTM"><a href="#Gradient-Descent-as-LSTM" class="headerlink" title="Gradient Descent as LSTM"></a>Gradient Descent as LSTM</h2><h3 id="V1"><a href="#V1" class="headerlink" title="V1"></a>V1</h3><h4 id="GD视为LSTM简化版："><a href="#GD视为LSTM简化版：" class="headerlink" title="GD视为LSTM简化版："></a>GD视为LSTM简化版：</h4><p>input &amp; forget gate的参数并非学习而来，而是人为设定</p><p><img src="image-20200204191826717.png" alt="image-20200204191826717"></p><h4 id="LSTM-for-GD"><a href="#LSTM-for-GD" class="headerlink" title="LSTM for GD"></a>LSTM for GD</h4><p>动态调整学习率并对先前参数进行处理</p><p><img src="image-20200204192217178.png" alt="image-20200204192217178"></p><h4 id="training"><a href="#training" class="headerlink" title="training"></a>training</h4><p><img src="image-20200204192420750.png" alt="image-20200204192420750"></p><h4 id="简化运算"><a href="#简化运算" class="headerlink" title="简化运算"></a>简化运算</h4><p><img src="image-20200204192639254.png" alt="image-20200204192639254"></p><h4 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h4><p>LSTM适用于所有参数的更新，仅有一个LSTM cell</p><p>training&amp;testing 应用不同model：CNN&amp;RNN (MAML无法做到)</p><p><img src="image-20200204192844628.png" alt="image-20200204192844628"></p><h3 id="v2"><a href="#v2" class="headerlink" title="v2"></a>v2</h3><p>下层LSTM类似于倒数（加速度）<br><img src="image-20200204193331312.png" alt="image-20200204193331312"></p><h2 id="Metric-based-Approach"><a href="#Metric-based-Approach" class="headerlink" title="Metric-based Approach"></a>Metric-based Approach</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><img src="image-20200204210726424.png" alt="image-20200204210726424"></p><h3 id="Face-Verification-amp-Face-Identification"><a href="#Face-Verification-amp-Face-Identification" class="headerlink" title="Face Verification &amp; Face Identification"></a>Face Verification &amp; Face Identification</h3><p>Face Identification:判断是一组人中哪一个</p><p>Face Verification:判断是否是同一个人</p><p><img src="image-20200204210930504.png" alt="image-20200204210930504"></p><h3 id="Face-Verification训练"><a href="#Face-Verification训练" class="headerlink" title="Face Verification训练"></a>Face Verification训练</h3><p>类似word2vec</p><p><img src="image-20200204211516701.png" alt="image-20200204211516701"></p><p><img src="image-20200204211728786.png" alt="image-20200204211728786"></p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p><img src="image-20200204214507405.png" alt="image-20200204214507405"></p><h4 id="Prototypical-Network"><a href="#Prototypical-Network" class="headerlink" title="Prototypical Network"></a>Prototypical Network</h4><p>效果大于Matching</p><p><img src="image-20200204214643877.png" alt="image-20200204214643877"></p><p>n-shots情景，对embedding取平均值：<br><img src="image-20200204214816320.png" alt="image-20200204214816320"></p><h4 id="Matching-Network"><a href="#Matching-Network" class="headerlink" title="Matching Network:"></a>Matching Network:</h4><p><img src="image-20200204214856730.png" alt="image-20200204214856730"></p><h4 id="Relation-Network"><a href="#Relation-Network" class="headerlink" title="Relation Network"></a>Relation Network</h4><p>不同于Prototypical Network &amp; Matching Network，Relation Network采用两个module，第一个用于提取embedding，第二个用于计算class与test set的embedding之间的相似度。</p><p>（彩色长方体为提取出的各类别embedding，黄色为输入embedding）</p><p>之前方法的相似度使用特定的计算方式（eg.余弦）</p><p><img src="image-20200204221345831.png" alt="image-20200204221345831"></p><h3 id="Imaginary-Data"><a href="#Imaginary-Data" class="headerlink" title="Imaginary Data"></a>Imaginary Data</h3><p>GAN网络部分与learning+prediction网络共同权重更新进行训练</p><p><img src="image-20200204222016936.png" alt="image-20200204222016936"></p><h2 id="Test-as-RNN"><a href="#Test-as-RNN" class="headerlink" title="Test as RNN"></a>Test as RNN</h2><h4 id="一般思路-general"><a href="#一般思路-general" class="headerlink" title="一般思路 general"></a>一般思路 general</h4><p>但实验中模型无法成功训练</p><p><img src="image-20200204222426498.png" alt="image-20200204222426498"></p><h3 id="MANN-amp-SNAIL"><a href="#MANN-amp-SNAIL" class="headerlink" title="MANN &amp; SNAIL"></a>MANN &amp; SNAIL</h3><p>SNAIL类似wavenet网络构造，加强输入之间的联系</p><p><img src="image-20200204222710008.png" alt="image-20200204222710008"></p>]]></content>
      
      
      <categories>
          
          <category> Meta Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Meta Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
